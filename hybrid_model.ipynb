{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "098d54a1-21be-40e0-b4ac-12d2e7cede2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "318/318 [==============================] - 25s 51ms/step - loss: 0.1084 - accuracy: 1.0000 - val_loss: 1.9986 - val_accuracy: 0.5000\n",
      "Epoch 2/10\n",
      "318/318 [==============================] - 7s 23ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 2.5125 - val_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "318/318 [==============================] - 7s 23ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 2.8341 - val_accuracy: 0.5000\n",
      "Epoch 4/10\n",
      "318/318 [==============================] - 7s 23ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.0721 - val_accuracy: 0.5000\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16, ResNet50\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# 1. Configure GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "# 2. Custom Data Generator for unstructured training data\n",
    "class CustomDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, image_dir, label_func, target_size=(224,224), batch_size=8, shuffle=True):\n",
    "        self.image_paths = [os.path.join(image_dir,f) for f in os.listdir(image_dir) \n",
    "                          if f.endswith(('.jpg','.jpeg','.png'))]\n",
    "        self.label_func = label_func\n",
    "        self.target_size = target_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.image_paths) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.image_paths[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        batch_images = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for path in batch_paths:\n",
    "            img = tf.keras.preprocessing.image.load_img(\n",
    "                path, target_size=self.target_size)\n",
    "            img = tf.keras.preprocessing.image.img_to_array(img)\n",
    "            img = img / 255.0  # Normalize\n",
    "            \n",
    "            label = self.label_func(os.path.basename(path))\n",
    "            \n",
    "            batch_images.append(img)\n",
    "            batch_labels.append(label)\n",
    "            \n",
    "        return np.array(batch_images), np.array(batch_labels)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.image_paths)\n",
    "\n",
    "# 3. Label function (modify according to your naming convention)\n",
    "def get_label(filename):\n",
    "    filename = filename.lower()\n",
    "    if 'autistic' in filename:\n",
    "        return 0\n",
    "    elif 'non_autistic' in filename or 'nonautistic' in filename:\n",
    "        return 1\n",
    "    else:\n",
    "        raise ValueError(f\"Cannot determine label for {filename}\")\n",
    "\n",
    "# 4. Hybrid Model\n",
    "def create_hybrid_model():\n",
    "    input_tensor = Input(shape=(224, 224, 3))\n",
    "    \n",
    "    # VGG16 branch\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "    vgg_out = GlobalAveragePooling2D()(vgg.output)\n",
    "    \n",
    "    # ResNet50 branch\n",
    "    resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=input_tensor)\n",
    "    resnet_out = GlobalAveragePooling2D()(resnet.output)\n",
    "    \n",
    "    # Combine features\n",
    "    combined = concatenate([vgg_out, resnet_out])\n",
    "    \n",
    "    # Classification head\n",
    "    output = Dense(1, activation='sigmoid')(combined)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    # Freeze base models\n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "    for layer in resnet.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 5. Training Process\n",
    "def train_model():\n",
    "    # Create generators\n",
    "    train_gen = CustomDataGenerator(\n",
    "        image_dir='./train',\n",
    "        label_func=get_label,\n",
    "        target_size=(224,224),\n",
    "        batch_size=8\n",
    "    )\n",
    "    \n",
    "    # Standard generator for validation (organized in subfolders)\n",
    "    valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    valid_gen = valid_datagen.flow_from_directory(\n",
    "        './valid',\n",
    "        target_size=(224,224),\n",
    "        batch_size=8,\n",
    "        class_mode='binary',\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # Verify we have data\n",
    "    if len(train_gen.image_paths) == 0:\n",
    "        raise ValueError(\"No images found in training directory\")\n",
    "    if valid_gen.samples == 0:\n",
    "        raise ValueError(\"No validation images found\")\n",
    "    \n",
    "    # Create and compile model\n",
    "    model = create_hybrid_model()\n",
    "    model.compile(\n",
    "        optimizer=Adam(0.0001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=valid_gen,\n",
    "        epochs=10,\n",
    "        callbacks=[\n",
    "            EarlyStopping(patience=3),\n",
    "            ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    model.save('final_model.h5')\n",
    "    print(\"Training completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da447d06-bb20-4442-ba77-04130e26371b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100 images belonging to 2 classes.\n",
      "\n",
      "Evaluating model on 100 validation images...\n",
      "\n",
      "Making predictions...\n",
      "12/12 [==============================] - 1s 36ms/step\n",
      "\n",
      "Validation samples: 96\n",
      "Correct predictions: 50\n",
      "Wrong predictions: 46\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50  0]\n",
      " [46  0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Autistic       0.52      1.00      0.68        50\n",
      "Non_Autistic       0.00      0.00      0.00        46\n",
      "\n",
      "    accuracy                           0.52        96\n",
      "   macro avg       0.26      0.50      0.34        96\n",
      "weighted avg       0.27      0.52      0.36        96\n",
      "\n",
      "\n",
      "AUC Score: 0.631\n",
      "\n",
      "Showing 5 correct predictions...\n",
      "\n",
      "Showing 5 wrong predictions...\n",
      "\n",
      "Evaluation complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mashr\\AppData\\Local\\Temp\\ipykernel_5128\\2236556151.py:49: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "W:\\anaconda\\envs\\tf210\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "W:\\anaconda\\envs\\tf210\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "W:\\anaconda\\envs\\tf210\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\mashr\\AppData\\Local\\Temp\\ipykernel_5128\\2236556151.py:67: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n",
      "C:\\Users\\mashr\\AppData\\Local\\Temp\\ipykernel_5128\\2236556151.py:80: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# 1. Load model and setup validation\n",
    "MODEL_PATH = 'final_model.h5'\n",
    "VALID_DIR = './valid'\n",
    "\n",
    "model = load_model(MODEL_PATH)\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
    "valid_gen = valid_datagen.flow_from_directory(\n",
    "    VALID_DIR,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=8,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "valid_steps = valid_gen.samples // valid_gen.batch_size\n",
    "\n",
    "# 2. Evaluation function\n",
    "def evaluate_model(model, generator, steps, class_names):\n",
    "    # Get predictions\n",
    "    print(\"\\nMaking predictions...\")\n",
    "    val_preds_prob = model.predict(generator, steps=steps, verbose=1)\n",
    "    val_preds = (val_preds_prob > 0.5).astype(int).flatten()\n",
    "    val_true = generator.classes[:len(val_preds)]\n",
    "    val_filenames = generator.filenames[:len(val_preds)]\n",
    "    \n",
    "    # Print basic metrics\n",
    "    print(f\"\\nValidation samples: {len(val_true)}\")\n",
    "    print(f\"Correct predictions: {(val_preds == val_true).sum()}\")\n",
    "    print(f\"Wrong predictions: {(val_preds != val_true).sum()}\")\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(val_true, val_preds)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=class_names, \n",
    "                yticklabels=class_names, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification Report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(val_true, val_preds, target_names=class_names))\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(val_true, val_preds_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print(f\"\\nAUC Score: {roc_auc:.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Example predictions\n",
    "    def plot_examples(indices, title, max_imgs=5):\n",
    "        plt.figure(figsize=(15,3))\n",
    "        for i, idx in enumerate(indices[:max_imgs]):\n",
    "            img_path = os.path.join(VALID_DIR, val_filenames[idx])\n",
    "            img = plt.imread(img_path)\n",
    "            plt.subplot(1, max_imgs, i+1)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"Pred: {class_names[val_preds[idx]]}\\nTrue: {class_names[val_true[idx]]}\")\n",
    "            plt.axis('off')\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "    \n",
    "    correct_idx = np.where(val_preds == val_true)[0]\n",
    "    wrong_idx = np.where(val_preds != val_true)[0]\n",
    "    \n",
    "    if len(correct_idx) > 0:\n",
    "        print(f\"\\nShowing {min(5, len(correct_idx))} correct predictions...\")\n",
    "        plot_examples(correct_idx, \"Correct Predictions\")\n",
    "    \n",
    "    if len(wrong_idx) > 0:\n",
    "        print(f\"\\nShowing {min(5, len(wrong_idx))} wrong predictions...\")\n",
    "        plot_examples(wrong_idx, \"Wrong Predictions\")\n",
    "\n",
    "# 3. Run evaluation\n",
    "print(f\"\\nEvaluating model on {valid_gen.samples} validation images...\")\n",
    "class_names = list(valid_gen.class_indices.keys())\n",
    "evaluate_model(model, valid_gen, valid_steps, class_names)\n",
    "print(\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "36ec7136-fef6-45c8-b5ae-62790880865c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def test_predictions(model, test_dir, img_size=(224, 224), batch_size=8):\n",
    "    \"\"\"\n",
    "    Comprehensive test set evaluation with visualization\n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        test_dir: Path to test directory\n",
    "        img_size: Target image size\n",
    "        batch_size: Batch size for prediction\n",
    "    \"\"\"\n",
    "    # 1. Setup test generator\n",
    "    test_filenames = [f for f in os.listdir(test_dir) \n",
    "                     if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if not test_filenames:\n",
    "        raise ValueError(f\"No images found in test directory: {test_dir}\")\n",
    "    \n",
    "    test_df = pd.DataFrame({'filename': test_filenames})\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_gen = test_datagen.flow_from_dataframe(\n",
    "        dataframe=test_df,\n",
    "        directory=test_dir,\n",
    "        x_col='filename',\n",
    "        y_col=None,\n",
    "        target_size=img_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=None,\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    # 2. Make predictions with simple progress indicator\n",
    "    print(f\"\\nPredicting on {len(test_filenames)} test images...\")\n",
    "    test_preds_prob = []\n",
    "    total_batches = len(test_gen)\n",
    "    \n",
    "    for i in range(total_batches):\n",
    "        batch = next(test_gen)\n",
    "        batch_preds = model.predict(batch, verbose=0)\n",
    "        test_preds_prob.extend(batch_preds.flatten().tolist())\n",
    "        print(f\"Processed batch {i+1}/{total_batches}\", end='\\r')\n",
    "    \n",
    "    test_preds = (np.array(test_preds_prob) > 0.5).astype(int)\n",
    "    \n",
    "    # 3. Display predictions\n",
    "    print(\"\\n\\nTest Set Predictions Summary:\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"{'Autistic':<15} {'Non_Autistic':<15} {'Total'}\")\n",
    "    print(\"-\"*50)\n",
    "    print(f\"{np.sum(test_preds == 0):<15} {np.sum(test_preds == 1):<15} {len(test_preds)}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # 4. Save predictions to CSV\n",
    "    results_df = pd.DataFrame({\n",
    "        'filename': test_gen.filenames,\n",
    "        'prediction': ['Autistic' if p == 0 else 'Non_Autistic' for p in test_preds],\n",
    "        'confidence': test_preds_prob,\n",
    "        'pred_raw': test_preds\n",
    "    })\n",
    "    \n",
    "    results_csv = 'test_predictions.csv'\n",
    "    results_df.to_csv(results_csv, index=False)\n",
    "    print(f\"\\nPredictions saved to {results_csv}\")\n",
    "    \n",
    "    # 5. Visualize sample predictions\n",
    "    plot_test_samples(test_dir, results_df, n_samples=min(10, len(test_filenames)))\n",
    "\n",
    "def plot_test_samples(test_dir, results_df, n_samples=10):\n",
    "    \"\"\"Visualize random test samples with predictions\"\"\"\n",
    "    samples = results_df.sample(n_samples) if len(results_df) > n_samples else results_df\n",
    "    \n",
    "    plt.figure(figsize=(20, 10))\n",
    "    for i, (_, row) in enumerate(samples.iterrows(), 1):\n",
    "        img_path = os.path.join(test_dir, row['filename'])\n",
    "        img = plt.imread(img_path)\n",
    "        \n",
    "        plt.subplot(2, 5, i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"{row['prediction']}\\nConf: {row['confidence']:.2f}\", \n",
    "                 color='green' if row['prediction'] == 'Non_Autistic' else 'red')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.suptitle('Sample Test Predictions', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def predict_single_image(model, img_path, img_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Make prediction on a single image with visualization\n",
    "    Args:\n",
    "        model: Trained Keras model\n",
    "        img_path: Path to image file\n",
    "        img_size: Target image size\n",
    "    \"\"\"\n",
    "    # 1. Load and preprocess image\n",
    "    if not os.path.exists(img_path):\n",
    "        raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "    \n",
    "    # Try face detection first\n",
    "    def detect_and_crop_face(img_path):\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            return None\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        (x, y, w, h) = faces[0]\n",
    "        return cv2.cvtColor(img[y:y+h, x:x+w], cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    img = detect_and_crop_face(img_path)\n",
    "    if img is None:\n",
    "        print(\"No face detected, using full image\")\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, img_size)\n",
    "    \n",
    "    # 2. Prepare for model input\n",
    "    x = np.expand_dims(img, axis=0) / 255.0\n",
    "    \n",
    "    # 3. Make prediction\n",
    "    pred_prob = model.predict(x, verbose=0)[0][0]\n",
    "    pred_label = 'Autistic' if pred_prob < 0.5 else 'Non_Autistic'\n",
    "    confidence = 1 - pred_prob if pred_label == 'Autistic' else pred_prob\n",
    "    \n",
    "    # 4. Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Prediction: {pred_label}\\nConfidence: {confidence:.3f}\",\n",
    "             fontsize=14, pad=20,\n",
    "             color='green' if pred_label == 'Non_Autistic' else 'red')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add confidence bar\n",
    "    ax = plt.gca()\n",
    "    ax.text(0.5, -0.1, \n",
    "            f\"\\nAutistic: {(1-pred_prob)*100:.1f}% | Non-Autistic: {pred_prob*100:.1f}%\", \n",
    "            transform=ax.transAxes,\n",
    "            ha='center', va='center', fontsize=12,\n",
    "            bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nPrediction Details:\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Image: {os.path.basename(img_path)}\")\n",
    "    print(f\"Prediction: {pred_label}\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Raw Score: {pred_prob:.3f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "# Example Usage:\n",
    "# test_predictions(hybrid_model, './test')\n",
    "# predict_single_image(hybrid_model, 'path/to/image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334bf670-5e16-4aa8-9ba0-b9e3384c180b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf210)",
   "language": "python",
   "name": "tf210"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
